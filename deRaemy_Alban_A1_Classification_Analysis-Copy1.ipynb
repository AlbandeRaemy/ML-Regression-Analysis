{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "Individual Case\n",
    "\n",
    "Alban de Raemy\n",
    "\n",
    "Classification Analysis\n",
    "\n",
    "MSBA 2\n",
    "\n",
    "Machine Learning\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Purpose of this Script</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script's purpose is to build a predictive model on the binomial variable (CROSS_SELL_SUCCESS). The goal is to develop reliable models, compare them and select the best model to predict which customer will subscribe to Apprentice Chef's news cross-selling promotion: Halfway There.\n",
    "My personal goal is to develop my skills in creating predictive models in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Analytical Objectives</h2><br>\n",
    "\n",
    "Part 1:\n",
    "Understand how much revenue Apprentice Chef should expect from each customer within their first year of using the services.\n",
    "\n",
    "Part 2: Predict which customer will subscribe to the Halfway there service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fundamental Data Exploration</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packaged necessary for this script. Setting the print options. Importing the dataset. Renaming one column with the wrong name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd                                     # data science essentials\n",
    "import matplotlib.pyplot as plt                         # essential graphical output\n",
    "import seaborn as sns                                   # enhanced graphical output\n",
    "from   scipy import stats                               # stats essentials\n",
    "import statsmodels.formula.api as smf                   # predictive modeling with nice outputs\n",
    "import gender_guesser.detector as gender                # guess gender based on (given) name\n",
    "import random as rand                                   # random\n",
    "import statsmodels.formula.api as smf                   # regression modeling\n",
    "from sklearn.model_selection import train_test_split    # train/test split\n",
    "from sklearn.linear_model import LinearRegression       # Linear Regression  \n",
    "from sklearn.neighbors import KNeighborsRegressor       # KNN for Regression\n",
    "from sklearn.preprocessing import StandardScaler        # standard scaler\n",
    "import sklearn.linear_model                             # Linear Regression\n",
    "from itertools import combinations                      # combinations of features\n",
    "from sklearn.linear_model import LogisticRegression     # logistic regression\n",
    "from sklearn.metrics import confusion_matrix            # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score               # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier      # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor       # KNN for regression\n",
    "from sklearn.model_selection import RandomizedSearchCV  # hyperparameter tuning\n",
    "from sklearn.metrics import make_scorer                 # customizable scorer\n",
    "from sklearn.ensemble import RandomForestClassifier     # random forest\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "\n",
    "# libraries for classification trees\n",
    "from sklearn.tree import DecisionTreeClassifier         # classification trees\n",
    "from sklearn.tree import export_graphviz                # exports graphics\n",
    "from six import StringIO                                # saves objects in memory\n",
    "from IPython.display import Image                       # displays on frontend\n",
    "import pydotplus  \n",
    "\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# setting random seed\n",
    "rand.seed(a = 219)\n",
    "\n",
    "# specifying file name for the dataset\n",
    "file = './Apprentice_Chef_Dataset.xlsx'\n",
    "\n",
    "#specifying file name for the dataset dictionary\n",
    "#dictionary = \"Apprentice_Chef_Data_Dictionary.xlsx\"\n",
    "\n",
    "\n",
    "# reading the file into Python\n",
    "chef = pd.read_excel(io=file)\n",
    "\n",
    "# reading the dictionary into Python\n",
    "#dict_df = pd.read_excel(dictionary)\n",
    "\n",
    "\n",
    "\n",
    "# using .shape to view (ROWS, COLUMNS)\n",
    "chef.shape\n",
    "\n",
    "\n",
    "\n",
    "# rename LARGEST_ORDER_SIZE to AVG_MEALS_ORDERED\n",
    "chef.rename(columns={'LARGEST_ORDER_SIZE': 'AVG_MEALS_ORDERED'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Our y-variable is REVENUE\n",
    "#sns.displot(data = chef,\n",
    "#           x = \"REVENUE\",\n",
    "#           height = 5,\n",
    "#           aspect = 2)\n",
    "# displaying the histogram\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our analysis with 1946 Observations and 28 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Missing Value Analysis and Imputation</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#checking if there is any missing data\n",
    "#chef.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only missing values in the family name variable (47 of them). I used the missing value flagger user-defined function to create a dummy variable. This feature ended up having a high p-value and was removed from the model. For the sake of script execution I remove this part since it's not relevant to our analysis.\n",
    "Additionaly, the customer with missing last names had their job title with their first name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classify Data</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>CROSS_SELL_SUCCESS</th>\n",
       "      <th>TOTAL_MEALS_ORDERED</th>\n",
       "      <th>UNIQUE_MEALS_PURCH</th>\n",
       "      <th>CONTACTS_W_CUSTOMER_SERVICE</th>\n",
       "      <th>PRODUCT_CATEGORIES_VIEWED</th>\n",
       "      <th>AVG_TIME_PER_SITE_VISIT</th>\n",
       "      <th>MOBILE_NUMBER</th>\n",
       "      <th>CANCELLATIONS_BEFORE_NOON</th>\n",
       "      <th>CANCELLATIONS_AFTER_NOON</th>\n",
       "      <th>TASTES_AND_PREFERENCES</th>\n",
       "      <th>PC_LOGINS</th>\n",
       "      <th>MOBILE_LOGINS</th>\n",
       "      <th>WEEKLY_PLAN</th>\n",
       "      <th>EARLY_DELIVERIES</th>\n",
       "      <th>LATE_DELIVERIES</th>\n",
       "      <th>PACKAGE_LOCKER</th>\n",
       "      <th>REFRIGERATED_LOCKER</th>\n",
       "      <th>AVG_PREP_VID_TIME</th>\n",
       "      <th>AVG_MEALS_ORDERED</th>\n",
       "      <th>MASTER_CLASSES_ATTENDED</th>\n",
       "      <th>MEDIAN_MEAL_RATING</th>\n",
       "      <th>AVG_CLICKS_PER_VISIT</th>\n",
       "      <th>TOTAL_PHOTOS_VIEWED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.0</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "      <td>1946.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2107.29</td>\n",
       "      <td>0.68</td>\n",
       "      <td>74.63</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.98</td>\n",
       "      <td>5.38</td>\n",
       "      <td>99.60</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.71</td>\n",
       "      <td>5.52</td>\n",
       "      <td>1.48</td>\n",
       "      <td>11.33</td>\n",
       "      <td>1.49</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.11</td>\n",
       "      <td>150.56</td>\n",
       "      <td>4.44</td>\n",
       "      <td>0.60</td>\n",
       "      <td>2.79</td>\n",
       "      <td>13.51</td>\n",
       "      <td>106.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1138.29</td>\n",
       "      <td>0.47</td>\n",
       "      <td>55.31</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.28</td>\n",
       "      <td>3.04</td>\n",
       "      <td>62.34</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>13.57</td>\n",
       "      <td>2.32</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.32</td>\n",
       "      <td>49.45</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.33</td>\n",
       "      <td>181.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>131.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>33.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1350.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>72.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>114.40</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1740.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>94.16</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>145.60</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2670.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>117.29</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>173.78</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>174.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8793.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>493.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>1645.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>52.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>564.20</td>\n",
       "      <td>11.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>1600.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       REVENUE  CROSS_SELL_SUCCESS  TOTAL_MEALS_ORDERED  UNIQUE_MEALS_PURCH  CONTACTS_W_CUSTOMER_SERVICE  PRODUCT_CATEGORIES_VIEWED  AVG_TIME_PER_SITE_VISIT  MOBILE_NUMBER  CANCELLATIONS_BEFORE_NOON  CANCELLATIONS_AFTER_NOON  TASTES_AND_PREFERENCES  PC_LOGINS  MOBILE_LOGINS  WEEKLY_PLAN  EARLY_DELIVERIES  LATE_DELIVERIES  PACKAGE_LOCKER  REFRIGERATED_LOCKER  AVG_PREP_VID_TIME  AVG_MEALS_ORDERED  MASTER_CLASSES_ATTENDED  MEDIAN_MEAL_RATING  AVG_CLICKS_PER_VISIT  TOTAL_PHOTOS_VIEWED\n",
       "count  1946.00             1946.00              1946.00              1946.0                      1946.00                    1946.00                  1946.00        1946.00                    1946.00                   1946.00                 1946.00    1946.00        1946.00      1946.00           1946.00          1946.00         1946.00              1946.00            1946.00            1946.00                  1946.00             1946.00               1946.00              1946.00\n",
       "mean   2107.29                0.68                74.63                 4.9                         6.98                       5.38                    99.60           0.88                       1.40                      0.17                    0.71       5.52           1.48        11.33              1.49             2.97            0.36                 0.11             150.56               4.44                     0.60                2.79                 13.51               106.43\n",
       "std    1138.29                0.47                55.31                 2.5                         2.28                       3.04                    62.34           0.33                       1.55                      0.43                    0.45       0.58           0.53        13.57              2.32             2.74            0.48                 0.32              49.45               1.55                     0.64                0.76                  2.33               181.01\n",
       "min     131.00                0.00                11.00                 1.0                         1.00                       1.00                    10.33           0.00                       0.00                      0.00                    0.00       4.00           0.00         0.00              0.00             0.00            0.00                 0.00              33.40               1.00                     0.00                1.00                  5.00                 0.00\n",
       "25%    1350.00                0.00                39.00                 3.0                         5.00                       3.00                    72.00           1.00                       0.00                      0.00                    0.00       5.00           1.00         1.00              0.00             1.00            0.00                 0.00             114.40               3.00                     0.00                2.00                 12.00                 0.00\n",
       "50%    1740.00                1.00                60.00                 5.0                         7.00                       5.00                    94.16           1.00                       1.00                      0.00                    1.00       6.00           1.00         7.00              0.00             2.00            0.00                 0.00             145.60               4.00                     1.00                3.00                 13.00                 0.00\n",
       "75%    2670.00                1.00                95.00                 7.0                         8.00                       8.00                   117.29           1.00                       2.00                      0.00                    1.00       6.00           2.00        13.00              3.00             4.00            1.00                 0.00             173.78               5.00                     1.00                3.00                 15.00               174.00\n",
       "max    8793.75                1.00               493.00                19.0                        18.00                      10.00                  1645.60           1.00                      13.00                      3.00                    1.00       7.00           3.00        52.00              9.00            19.00            1.00                 1.00             564.20              11.00                     3.00                5.00                 19.00              1600.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics for numeric data\n",
    "chef.describe(include='number').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the descriptive statistics above, I categorized the varibles into continuous, interval, categorical."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CONTINUOUS\n",
    "----------\n",
    "REVENUE\n",
    "AVG_TIME_PER_SITE_VISIT\t\n",
    "WEEKLY_PLAN\t(Number of weeks the customer subscribed to weekly plan)\n",
    "AVG_PREP_VID_TIME (avg. time in second meal prep instruction video was playing)\n",
    "AVG_MEALS_ORDERED (Avg_number of meals ordered by customer)\n",
    "AVG_CLICKS_PER_VISIT\n",
    "TOTAL_PHOTOS_VIEWED\n",
    "\n",
    "\n",
    "INTERVAL / COUNT\n",
    "----------------\n",
    "TOTAL_MEALS_ORDERED\n",
    "AVG_MEALS_ORDERED\n",
    "MEDIAN_MEAL_RATING\n",
    "UNIQUE_MEALS_PURCH\n",
    "CONTACTS_W_CUSTOMER_SERVICE\n",
    "PRODUCT_CATEGORIES_VIEWED\n",
    "CANCELLATIONS_BEFORE_NOON\n",
    "CANCELLATIONS_AFTER_NOON\n",
    "PC_LOGINS\n",
    "MOBILE_LOGINS\n",
    "EARLY_DELIVERIES\n",
    "LATE_DELIVERIES\t\n",
    "MASTER_CLASSES_ATTENDED\n",
    "\n",
    "\n",
    "CATEGORICAL\n",
    "-----------\n",
    "MOBILE_NUMBER (1= mobile, 0 = landline)\n",
    "PACKAGE_LOCKER\n",
    "REFRIGERATED_LOCKER\t\n",
    "CROSS_SELL_SUCCESS\n",
    "TASTES_AND_PREFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visual Data Exploration</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created histograms and boxplots for each features to explore their distribution visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Creating histograms and bar plots of each feature to better understand their distribution\n",
    "# subsetting data so that the there's into categorical/descriptive data\n",
    "chef_num = chef.drop([\"NAME\", \"EMAIL\", \"FIRST_NAME\", \"FAMILY_NAME\"], axis = 1)\n",
    "chef_cat = chef.select_dtypes(include = 'object')\n",
    "\n",
    "\n",
    "# Look at histogram of the numerical variables to better understand their distribution\n",
    "#for i in chef_num.columns:\n",
    "#    def plot(x, y, z):\n",
    "#        fig, ax = plt.subplots(figsize = (12,12))\n",
    "#        plt.subplot(2,2,y)\n",
    "#        sns.distplot(chef[x],\n",
    "#            color = z)\n",
    "#        plt.xlabel(x)\n",
    "#        plt.title(x)\n",
    "#        plt.show()\n",
    "#        return\n",
    "#    plot(i, 1, \"b\")\n",
    "\n",
    "\n",
    "# Look at boxplot of numerical variables to better understand their distribution\n",
    "# sns.boxplot(x=chef['AVG_PREP_VID_TIME'])\n",
    "\n",
    "# creating a list of continuous features (including REVENUE)\n",
    "#continuous_data = ['REVENUE','AVG_TIME_PER_SITE_VISIT', 'WEEKLY_PLAN', 'AVG_PREP_VID_TIME',\n",
    "#                   'AVG_MEALS_ORDERED', 'AVG_CLICKS_PER_VISIT', 'TOTAL_PHOTOS_VIEWED']\n",
    "\n",
    "\n",
    "# developing a correlation matrix based on continuous features\n",
    "#chef_corr = chef[continuous_data].corr(method = 'pearson')\n",
    "\n",
    "\n",
    "# filtering the results to only show correlations with CROSS_SALE_SUCCESS\n",
    "#chef_corr.loc[ : , 'CROSS_SALE_SUCCESS'].round(decimals = 2).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested the correlation with the continuous features we found. As of now, the highest correlation value her, was AVG_PREP_VID_TIME and TOTAL_PHOTOS_VIEWED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Transformation</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>User Defined Function (Created in Class)</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define the functions we created in class that I will use later on in my script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# optimal_neighbors and visual_cm\n",
    "########################################\n",
    "def optimal_neighbors(X_data,\n",
    "                      y_data,\n",
    "                      standardize = True,\n",
    "                      pct_test=0.25,\n",
    "                      seed=219,\n",
    "                      response_type='reg',\n",
    "                      max_neighbors=20,\n",
    "                      show_viz=True):\n",
    "    \"\"\"\n",
    "Exhaustively compute training and testing results for KNN across\n",
    "[1, max_neighbors]. Outputs the maximum test score and (by default) a\n",
    "visualization of the results.\n",
    "PARAMETERS\n",
    "----------\n",
    "X_data        : explanatory variable data\n",
    "y_data        : response variable\n",
    "standardize   : whether or not to standardize the X data, default True\n",
    "pct_test      : test size for training and validation from (0,1), default 0.25\n",
    "seed          : random seed to be used in algorithm, default 219\n",
    "response_type : type of neighbors algorithm to use, default 'reg'\n",
    "    Use 'reg' for regression (KNeighborsRegressor)\n",
    "    Use 'class' for classification (KNeighborsClassifier)\n",
    "max_neighbors : maximum number of neighbors in exhaustive search, default 20\n",
    "show_viz      : display or surpress k-neigbors visualization, default True\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "    if standardize == True:\n",
    "        # optionally standardizing X_data\n",
    "        scaler             = StandardScaler()\n",
    "        scaler.fit(X_data)\n",
    "        X_scaled           = scaler.transform(X_data)\n",
    "        X_scaled_df        = pd.DataFrame(X_scaled)\n",
    "        X_data             = X_scaled_df\n",
    "\n",
    "\n",
    "\n",
    "    # train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data,\n",
    "                                                        y_data,\n",
    "                                                        test_size = pct_test,\n",
    "                                                        random_state = seed)\n",
    "\n",
    "\n",
    "    # creating lists for training set accuracy and test set accuracy\n",
    "    training_accuracy = []\n",
    "    test_accuracy = []\n",
    "    \n",
    "    \n",
    "    # setting neighbor range\n",
    "    neighbors_settings = range(1, max_neighbors + 1)\n",
    "\n",
    "\n",
    "    for n_neighbors in neighbors_settings:\n",
    "        # building the model based on response variable type\n",
    "        if response_type == 'reg':\n",
    "            clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)\n",
    "            \n",
    "        elif response_type == 'class':\n",
    "            clf = KNeighborsClassifier(n_neighbors = n_neighbors)\n",
    "            clf.fit(X_train, y_train)            \n",
    "            \n",
    "        else:\n",
    "            print(\"Error: response_type must be 'reg' or 'class'\")\n",
    "        \n",
    "        \n",
    "        # recording the training set accuracy\n",
    "        training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "        # recording the generalization accuracy\n",
    "        test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "    # optionally displaying visualization\n",
    "    if show_viz == True:\n",
    "        # plotting the visualization\n",
    "        fig, ax = plt.subplots(figsize=(12,8))\n",
    "        plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "        plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"n_neighbors\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    # returning optimal number of neighbors\n",
    "    print(f\"The optimal number of neighbors is: {test_accuracy.index(max(test_accuracy))+1}\")\n",
    "    return test_accuracy.index(max(test_accuracy))+1\n",
    "\n",
    "\n",
    "########################################\n",
    "# visual_cm\n",
    "########################################\n",
    "def visual_cm(true_y, pred_y, labels = None):\n",
    "    \"\"\"\n",
    "Creates a visualization of a confusion matrix.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "true_y : true values for the response variable\n",
    "pred_y : predicted values for the response variable\n",
    "labels : , default None\n",
    "    \"\"\"\n",
    "    # visualizing the confusion matrix\n",
    "\n",
    "    # setting labels\n",
    "    lbls = labels\n",
    "    \n",
    "\n",
    "    # declaring a confusion matrix object\n",
    "    cm = confusion_matrix(y_true = true_y,\n",
    "                          y_pred = pred_y)\n",
    "\n",
    "\n",
    "    # heatmap\n",
    "    sns.heatmap(cm,\n",
    "                annot       = True,\n",
    "                xticklabels = lbls,\n",
    "                yticklabels = lbls,\n",
    "                cmap        = 'Blues',\n",
    "                fmt         = 'g')\n",
    "\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix of the Classifier')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# text_split_feature\n",
    "#########################\n",
    "def text_split_feature(col, df, sep=' ', new_col_name='number_of_names'):\n",
    "    \"\"\"\n",
    "Splits values in a string Series (as part of a DataFrame) and sums the number\n",
    "of resulting items. Automatically appends summed column to original DataFrame.\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "col          : column to split\n",
    "df           : DataFrame where column is located\n",
    "sep          : string sequence to split by, default ' '\n",
    "new_col_name : name of new column after summing split, default\n",
    "               'number_of_names'\n",
    "\"\"\"\n",
    "    \n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    \n",
    "    for index, val in df.iterrows():\n",
    "        df.loc[index, new_col_name] = len(df.loc[index, col].split(sep = ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Split for Names</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a new feature based on the number of words that each value of Name has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split_feature('NAME', chef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": []
   },
   "source": [
    "<h3>Preparing Explanatory and Response Data</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_data = chef.drop('CROSS_SELL_SUCCESS', axis=1)\n",
    "chef_target = chef.loc[:, 'CROSS_SELL_SUCCESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chef_num = chef.drop([\"NAME\", \"EMAIL\", \"FIRST_NAME\", \"FAMILY_NAME\"], axis = 1)\n",
    "chef_cat = chef.select_dtypes(include = 'object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Outlier Startegy</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by looking at histograms and boxplots of all numerical variables to have a better understanding of their distribution. I used the Z-score method to detect and highlight outliers. I created new columns for each variables and their outliers. I then highlighted in my dummy variables which values had an absolute Z-score higer than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Outlier detection and strategy\n",
    "\n",
    "# create a df where values with an absolute Z-score higher than 3 (outliers)\n",
    "outlier_df = pd.DataFrame(np.abs(stats.zscore(chef_num)) > 3, columns=chef_num.columns)\n",
    "outlier_columns = outlier_df.sum(axis=0).loc[(outlier_df.sum(axis=0) > 0)].index\n",
    "\n",
    "#create additional columns with the outliers for each variables, with out_ as prefix\n",
    "outlier_features = outlier_df[outlier_columns].astype('int').add_prefix('out_')\n",
    "chef_num = chef_num.join(outlier_features) #join the new columns to our chef_num df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Log Transformation</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used log transformation method that creates distribution easier to work with and more normally\n",
    "distributed. The benefits of log transformation are described in this [article](\n",
    "https://medium.com/@kyawsawhtoon/log-transformation-purpose-and-interpretation-9444b4b049c9). Ater this transformation, I dropped the original columns from my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "log_chef_col = ['REVENUE',\n",
    "                'AVG_TIME_PER_SITE_VISIT',\n",
    "                'WEEKLY_PLAN',\n",
    "                'AVG_PREP_VID_TIME',\n",
    "                'AVG_MEALS_ORDERED',\n",
    "                'AVG_CLICKS_PER_VISIT',\n",
    "                'TOTAL_PHOTOS_VIEWED',\n",
    "                'TOTAL_MEALS_ORDERED',\n",
    "                'AVG_MEALS_ORDERED',\n",
    "                'MEDIAN_MEAL_RATING',\n",
    "                'UNIQUE_MEALS_PURCH',\n",
    "                'CONTACTS_W_CUSTOMER_SERVICE',\n",
    "                'PRODUCT_CATEGORIES_VIEWED',\n",
    "                'CANCELLATIONS_BEFORE_NOON',\n",
    "                'CANCELLATIONS_AFTER_NOON',\n",
    "                'PC_LOGINS',\n",
    "                'MOBILE_LOGINS',\n",
    "                'EARLY_DELIVERIES',\n",
    "                'LATE_DELIVERIES',\n",
    "                'MASTER_CLASSES_ATTENDED',\n",
    "               ]\n",
    "# for loop to create new columns with the log transformation\n",
    "for col in log_chef_col:\n",
    "    chef_num['log_'+col] = np.log(chef_num[col]+1) #add 1 to avoid doing log of 0\n",
    "    \n",
    "# removing original columns now that we have the log columns\n",
    "chef_num.drop(log_chef_col, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dummy variable Photos viewed</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exploring the distribution visually of our new log variables with the help of scatter plots and box plots,\n",
    "we noticed that the variable log_TOTAL_PHOTOS_VIEWED still had a clear separation of customer who viewed \n",
    "photos and customer who didn't. For that reason, I decided to separate them with a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#PHOTOS_VIEWED_zeros = len(chef_num['log_TOTAL_PHOTOS_VIEWED'][chef_num['log_TOTAL_PHOTOS_VIEWED'] == 0])\n",
    "#TOTAL_MEALS_zeros = len(chef_num['log_TOTAL_MEALS_ORDERED'][chef_num['log_TOTAL_MEALS_ORDERED'] == 0])\n",
    "#TIME_SPENT_zeros = len(chef_num['log_AVG_TIME_PER_SITE_VISIT'][chef_num['log_AVG_TIME_PER_SITE_VISIT'] == 0])\n",
    "#UNIQUE_MEALS_zeros = len(chef_num['log_UNIQUE_MEALS_PURCH'][chef_num['log_UNIQUE_MEALS_PURCH'] == 0])\n",
    "\n",
    "#print(f\"\"\"\n",
    "#                             No\\t\\tYes\n",
    "#                           ---------------------\n",
    "#Total Photos Viewed       | {PHOTOS_VIEWED_zeros}\\t\\t{len(chef) - PHOTOS_VIEWED_zeros}\n",
    "#Total Meals Ordered       | {TOTAL_MEALS_zeros}\\t\\t{len(chef) - TOTAL_MEALS_zeros}\n",
    "#Avg Time per visit        | {TIME_SPENT_zeros}\\t\\t{len(chef) - TIME_SPENT_zeros}\n",
    "#Unique meals purchased    | {UNIQUE_MEALS_zeros}\\t\\t{len(chef) - UNIQUE_MEALS_zeros}\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the chart (if we remove the comment), only Total Photos Viewed has a significant amount of \n",
    "values in the \"No\" column. I will separate both values in a dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#dummy variable for viewing photos\n",
    "chef_num['VIEWED_PHOTOS'] = 0\n",
    "\n",
    "for index, value in chef_num.iterrows():\n",
    "    \n",
    "    if chef_num.loc[index, 'log_TOTAL_PHOTOS_VIEWED'] >0:\n",
    "        chef_num.loc[index, 'VIEWED_PHOTOS'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Categorize Email domains into groups</h3><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, the goal is to categorize the email domain in different groups. We start by separating the domain from the rest of the email address. We then create lists to separate the domains. We finish by creating dummy variables for each group of emails (and dropping the original email column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# STEP 1: splitting personal emails into Personal, Professional, and Junk \n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in chef_cat.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = chef_cat.loc[index, 'EMAIL'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "\n",
    "# STEP 2: concatenating with original DataFrame\n",
    "\n",
    "# safety measure in case of multiple concatenations\n",
    "# remove? chef = pd.read_excel(file)\n",
    "\n",
    "\n",
    "# renaming column to concatenate\n",
    "email_df.columns = ['0' , 'personal_email_domain']\n",
    "\n",
    "\n",
    "# concatenating personal_email_domain with chef DataFrame\n",
    "chef_cat = pd.concat([chef_cat, email_df['personal_email_domain']],\n",
    "                   axis = 1)\n",
    "\n",
    "# set email domain types\n",
    "professional_email_domains = ['@mmm.com', '@amex.com', '@apple.com',\n",
    "                                '@boeing.com', '@caterpillar.com', '@chevron.com',\n",
    "                                '@cisco.com', '@cocacola.com', '@disney.com', \n",
    "                                '@dupont.com', '@exxon.com', '@ge.org', '@goldmansacs.com',\n",
    "                                '@homedepot.com', '@ibm.com', '@intel.com', '@jnj.com', \n",
    "                                '@jpmorgan.com','@mcdonalds.com', '@merck.com', '@microsoft.com',\n",
    "                                '@nike.com', '@pfizer.com', '@pg.com', '@travelers.com',\n",
    "                                '@unitedtech.com', '@unitedhealth.com', '@verizon.com', \n",
    "                                '@visa.com', '@walmart.com']\n",
    "junk_email_domains = ['@me.com', '@aol.com', '@hotmail.com', '@live.com', '@msn.com', '@passport.com']\n",
    "personal_email_domains  = ['@gmail.com', '@yahoo.com', '@protonmail.com']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "\n",
    "# looping to group observations by domain type\n",
    "for domain in email_df['personal_email_domain']:\n",
    "        if '@' + domain in professional_email_domains:\n",
    "            placeholder_lst.append('Professional')\n",
    "            \n",
    "        elif '@' + domain in personal_email_domains:\n",
    "            placeholder_lst.append('Personal')\n",
    "            \n",
    "        elif '@' + domain in junk_email_domains:\n",
    "            placeholder_lst.append('Junk')\n",
    "            \n",
    "        else:\n",
    "            placeholder_lst.append('Unknown')\n",
    "\n",
    "\n",
    "# concatenating with original DataFrame\n",
    "chef_cat['domain_group'] = pd.Series(placeholder_lst)\n",
    "\n",
    "\n",
    "# checking results\n",
    "# chef_cat['domain_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the emails fall into these 3 categories which make it easier to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# STEP 2: Creating a new variables for each group of email domain\n",
    "\n",
    "########\n",
    "#creating a bool variable for each domain type\n",
    "########\n",
    "\n",
    "#creating a placeholder list that will be used by the following three loops\n",
    "plc_holder_list_pers = []\n",
    "plc_holder_list_prof = []\n",
    "plc_holder_list_junk = []\n",
    "\n",
    "\n",
    "#creating a variable \"Personal_Domain\" for personal email\n",
    "for i in chef_cat[\"domain_group\"]:\n",
    "    if i == \"Personal\":\n",
    "        plc_holder_list_pers.append(1)\n",
    "    else: \n",
    "        plc_holder_list_pers.append(0)\n",
    "        \n",
    "chef_cat[\"Personal_Domain\"] = pd.Series(plc_holder_list_pers)\n",
    "\n",
    "#creating a variable \"Professional_Domain\" for professional email\n",
    "for i in chef_cat[\"domain_group\"]:\n",
    "    if i == \"Professional\":\n",
    "        plc_holder_list_prof.append(1)\n",
    "    else:\n",
    "        plc_holder_list_prof.append(0)\n",
    "        \n",
    "chef_cat[\"Professional_Domain\"] = pd.Series(plc_holder_list_prof)\n",
    "\n",
    "#creating a variable \"Junk_Domain\" for Junk email\n",
    "for i in chef_cat[\"domain_group\"]:\n",
    "    if i == \"Junk\":\n",
    "        plc_holder_list_junk.append(1)\n",
    "    else:\n",
    "        plc_holder_list_junk.append(0)\n",
    "        \n",
    "chef_cat[\"Junk\"] = pd.Series(plc_holder_list_junk)\n",
    "\n",
    "#dropping un-needed columns\n",
    "chef_cat = chef_cat.drop([\"NAME\", \"EMAIL\", \"FAMILY_NAME\", \"domain_group\", \"personal_email_domain\"],\n",
    "             axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gender Guesser</h3><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After observing that close to 70% of the gender where unknown even after manually modifying the name that appears multiple times so they get read correctly, we still get a poor results. Additionlay, the gender.guesser takes a very long time to process so I saved the result in a csv file but it adds considerable time for python to read it. For these reasons, I decided to drop the analysis of this variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering</h2><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# cleaning datasets from unnecessary features\n",
    "chef_jo = chef_cat.join(chef_num)\n",
    "\n",
    "# drop revenue and strings variables\n",
    "chef_var = chef_jo.drop([\"CROSS_SELL_SUCCESS\", \"FIRST_NAME\"], axis = 1)\n",
    "chef_model = chef_jo[['CROSS_SELL_SUCCESS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Creation of additional interesting features</h3><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I created additional varibales based on the existing one to further analyze our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Total Photos viewed per login (pc + mobile)\n",
    "chef_var['PHOTOS_VIEWED_PER_LOGIN'] = chef_var['log_TOTAL_PHOTOS_VIEWED'] / (chef_var['log_MOBILE_LOGINS'] + chef_var['log_PC_LOGINS'])\n",
    "\n",
    "# Mater class attended per avg prep vid time\n",
    "chef_var['NUMN_CLASS_PER_PREP_TIME'] = chef_var['log_MASTER_CLASSES_ATTENDED'] / chef_var['log_AVG_PREP_VID_TIME']\n",
    "\n",
    "\n",
    "# average number of orders per time/visit\n",
    "chef_var['AVG_NUMBR_OR_PER_TIME_VISIT'] = chef_var['log_AVG_MEALS_ORDERED'] / chef_var['log_AVG_TIME_PER_SITE_VISIT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Combination of Features</h3><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Create combination of features \n",
    "\n",
    "interaction_list = ['log_AVG_TIME_PER_SITE_VISIT',\n",
    "'log_WEEKLY_PLAN',\n",
    "'log_CONTACTS_W_CUSTOMER_SERVICE',\n",
    "'log_AVG_PREP_VID_TIME',\n",
    "'log_AVG_MEALS_ORDERED',\n",
    "'log_AVG_CLICKS_PER_VISIT',\n",
    "'log_TOTAL_PHOTOS_VIEWED',\n",
    "'log_TOTAL_MEALS_ORDERED',\n",
    "'log_MEDIAN_MEAL_RATING',\n",
    "'log_UNIQUE_MEALS_PURCH',\n",
    "'log_CONTACTS_W_CUSTOMER_SERVICE',\n",
    "'log_PRODUCT_CATEGORIES_VIEWED',\n",
    "'log_CANCELLATIONS_BEFORE_NOON',\n",
    "'log_CANCELLATIONS_AFTER_NOON',\n",
    "'log_PC_LOGINS',\n",
    "'log_MOBILE_LOGINS',\n",
    "'log_EARLY_DELIVERIES',\n",
    "'log_LATE_DELIVERIES',\n",
    "'log_MASTER_CLASSES_ATTENDED',\n",
    "'PHOTOS_VIEWED_PER_LOGIN',\n",
    "'NUMN_CLASS_PER_PREP_TIME',  \n",
    "'AVG_NUMBR_OR_PER_TIME_VISIT'\n",
    "]\n",
    "\n",
    "interactions = list(combinations(interaction_list, 2))\n",
    "\n",
    "for interaction in interactions:\n",
    "    chef_var['{}_{}'.format(interaction_list[0], interaction[1])] = chef_var[interaction[0]]* chef_var[interaction[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've added quite a few new features, let's see how they impact our model. Adding a variety of interactions between variables is likely to increase the score of the models. I used the method discussed in this [article](https://towardsdatascience.com/feature-engineering-combination-polynomial-features-3caa4c77a755) from Towards Data Science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Standardize the data</h3><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we standarize the data with the function Standard Scaler. This [article](https://towardsdatascience.com/how-and-why-to-standardize-your-data-996926c2c832) from towards data science highlights the benefits of standarizing our data. Mainly it scales our data from 0 to 1 and help machine learning models achieve better results due to the similarity in scale and the more normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Creating a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Fitting the scaler with the data\n",
    "scaler.fit(chef_var)\n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(chef_var)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "# adding labels to the scaled DataFrame\n",
    "X_scaled_df.columns = chef_var.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "chef_data = X_scaled_df\n",
    "chef_target = chef_model\n",
    "chef_total = X_scaled_df.join(chef_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Building Predictive Models</h2><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explore the data, and modified it in a way to make it more rich and suited for modeling we are ready to start!\n",
    "\n",
    "The best way to evaluate a classification model performance (at least for the ones we will use) is the Area Under the Curve also know as AUC. This metric is much more reliabel than for example accuracy because it takes into account to fundamentals factors: sensitivity and Specificity. \n",
    "\n",
    "We will use many different model to see which one performs the best with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Correlation</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running a correlation on our data and CROSS_SELL_SUCCESS, we can better understand the realtionship between features and our y-variable. We can see that the strongest correaltion is with the variable Junk at -0.28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#df_corr  = chef_total.corr().round(2)\n",
    "#df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train-Split our data, with 25% test size and 75% train size</h3><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the performance of our models we need to run them on different data than the one they were trained on. Additionaly, we need to make sure our y-variable is startified and has the same proportions in the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "\n",
    "# merging training data for statsmodels\n",
    "chef_train = pd.concat([X_train, y_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# here we make sure the startification worked\n",
    "\n",
    "#print(f\"\"\"\n",
    "\n",
    "#Response Variable Proportions (Training Set)\n",
    "#--------------------------------------------\n",
    "#{y_train.value_counts(normalize = True).round(decimals = 2)}\n",
    "\n",
    "\n",
    "\n",
    "#Response Variable Proportions (Testing Set)\n",
    "#--------------------------------------------\n",
    "#{y_test.value_counts(normalize = True).round(decimals = 2)}\n",
    "#\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression</h3><br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# printing the columns names to easily copy/paste them in our model\n",
    "#for val in chef_data:\n",
    "#    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running our regression with all variables we remove all the features that have an insignificant p-value (lower than 0.05). We do that one by one, because each feature remove influence the p-values of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# instantiating a logistic regression model object\n",
    "#logistic_full = smf.logit(formula = \"\"\"  CROSS_SELL_SUCCESS ~ \n",
    "# Professional_Domain + \n",
    "# Junk + \n",
    "# MOBILE_NUMBER + \n",
    "# TASTES_AND_PREFERENCES + \n",
    "# number_of_names +    \n",
    "# out_number_of_names +  \n",
    "# log_CANCELLATIONS_BEFORE_NOON +  \n",
    "# log_MOBILE_LOGINS +  \n",
    "# log_AVG_TIME_PER_SITE_VISIT_log_AVG_PREP_VID_TIME +  \n",
    "# log_AVG_TIME_PER_SITE_VISIT_log_MOBILE_LOGINS +  \n",
    "# log_AVG_TIME_PER_SITE_VISIT_log_LATE_DELIVERIES \"\"\",\n",
    "#                                         data    = chef_train)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "#results_full = logistic_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "#results_full.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression with Scikit Learn</h3><br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After carefully removing features with insignificant P-Values, we create different dictionaries to store the names of the features that impact our models. After creating 3 different dictionaries, the ones that had the best result was the 2nd one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# creating a dictionaries to store chef models variables\n",
    "\n",
    "chef1_dict = {\n",
    "\n",
    " # full model\n",
    " 'logit_full'   : [  'Personal_Domain', 'Professional_Domain', 'Junk' \n",
    " 'MOBILE_NUMBER', \n",
    " 'TASTES_AND_PREFERENCES', \n",
    " 'PACKAGE_LOCKER', \n",
    " 'REFRIGERATED_LOCKER', \n",
    " 'm_FAMILY_NAME', \n",
    " 'number_of_names', \n",
    " 'out_REVENUE',\n",
    " 'out_TOTAL_MEALS_ORDERED', \n",
    " 'out_UNIQUE_MEALS_PURCH', \n",
    " 'out_CONTACTS_W_CUSTOMER_SERVICE', \n",
    " 'out_AVG_TIME_PER_SITE_VISIT', \n",
    " 'out_CANCELLATIONS_BEFORE_NOON', \n",
    " 'out_CANCELLATIONS_AFTER_NOON', \n",
    " 'out_EARLY_DELIVERIES', \n",
    " 'out_LATE_DELIVERIES', \n",
    " 'out_AVG_PREP_VID_TIME', \n",
    " 'out_AVG_MEALS_ORDERED', \n",
    " 'out_MASTER_CLASSES_ATTENDED', \n",
    " 'out_AVG_CLICKS_PER_VISIT', \n",
    " 'out_TOTAL_PHOTOS_VIEWED', \n",
    " 'out_m_FAMILY_NAME', \n",
    " 'out_number_of_names', \n",
    " 'log_REVENUE', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT', \n",
    " 'log_WEEKLY_PLAN', \n",
    " 'log_AVG_PREP_VID_TIME', \n",
    " 'log_AVG_MEALS_ORDERED', \n",
    " 'log_AVG_CLICKS_PER_VISIT', \n",
    " 'log_TOTAL_PHOTOS_VIEWED', \n",
    " 'log_TOTAL_MEALS_ORDERED', \n",
    " 'log_MEDIAN_MEAL_RATING', \n",
    " 'log_UNIQUE_MEALS_PURCH', \n",
    " 'log_CONTACTS_W_CUSTOMER_SERVICE', \n",
    " 'log_PRODUCT_CATEGORIES_VIEWED', \n",
    " 'log_CANCELLATIONS_BEFORE_NOON', \n",
    " 'log_CANCELLATIONS_AFTER_NOON', \n",
    " 'log_PC_LOGINS', \n",
    " 'log_MOBILE_LOGINS', \n",
    " 'log_EARLY_DELIVERIES', \n",
    " 'log_LATE_DELIVERIES', \n",
    " 'log_MASTER_CLASSES_ATTENDED', \n",
    " 'VIEWED_PHOTOS', \n",
    " 'PHOTOS_VIEWED_PER_LOGIN', \n",
    " 'NUMN_CLASS_PER_PREP_TIME', \n",
    " 'AVG_NUMBR_OR_PER_TIME_VISIT', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_WEEKLY_PLAN', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_CONTACTS_W_CUSTOMER_SERVICE', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_AVG_PREP_VID_TIME', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_AVG_MEALS_ORDERED', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_AVG_CLICKS_PER_VISIT', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_TOTAL_PHOTOS_VIEWED', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_TOTAL_MEALS_ORDERED', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_MEDIAN_MEAL_RATING', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_UNIQUE_MEALS_PURCH', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_PRODUCT_CATEGORIES_VIEWED', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_CANCELLATIONS_BEFORE_NOON', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_CANCELLATIONS_AFTER_NOON', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_PC_LOGINS', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_MOBILE_LOGINS', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_EARLY_DELIVERIES', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_LATE_DELIVERIES', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_MASTER_CLASSES_ATTENDED', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_PHOTOS_VIEWED_PER_LOGIN', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_NUMN_CLASS_PER_PREP_TIME', \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_AVG_NUMBR_OR_PER_TIME_VISIT'],\n",
    " \n",
    "\n",
    " # significant variables only (set 1)\n",
    " 'logit_sig'    : [ 'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'TASTES_AND_PREFERENCES',\n",
    "                   'PC_LOGINS', 'EARLY_DELIVERIES', 'MEDIAN_MEAL_RATING'],\n",
    "    \n",
    "    \n",
    " # significant variables only (set 2)\n",
    " 'logit_sig_2'  : [  'Professional_Domain', 'Junk', 'MOBILE_NUMBER', \n",
    " 'TASTES_AND_PREFERENCES', 'number_of_names', 'out_number_of_names',  \n",
    " 'log_CANCELLATIONS_BEFORE_NOON', 'log_MOBILE_LOGINS',  \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_AVG_PREP_VID_TIME',  \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_MOBILE_LOGINS',  \n",
    " 'log_AVG_TIME_PER_SITE_VISIT_log_LATE_DELIVERIES']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# printing chef1 variable sets\n",
    "#print(f\"\"\"\n",
    "#/--------------------------\\\\\n",
    "#|Explanatory Variable Sets |\n",
    "#\\\\--------------------------/\n",
    "\n",
    "#Full Model:\n",
    "#-----------\n",
    "#{chef1_dict['logit_full']}\n",
    "\n",
    "\n",
    "#First Significant p-value Model:\n",
    "#--------------------------------\n",
    "#{chef1_dict['logit_sig']}\n",
    "\n",
    "\n",
    "#Second Significant p-value Model:\n",
    "#---------------------------------\n",
    "#{chef1_dict['logit_sig_2']}\n",
    "#\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use our different dictionaries on logreg to see which one performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Training ACCURACY: 0.7457\n",
      "LogReg Testing  ACCURACY: 0.7454\n",
      "LogReg Train-Test Gap   : 0.0003\n"
     ]
    }
   ],
   "source": [
    "# train/test split with the full model\n",
    "chef_data   =  chef_total.loc[ : , chef1_dict['logit_sig_2']]\n",
    "chef_target =  chef_total.loc[ : , 'CROSS_SELL_SUCCESS']\n",
    "\n",
    "\n",
    "# this is the exact code we were using before\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            chef_data,\n",
    "            chef_target,\n",
    "            random_state = 219,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = chef_target)\n",
    "\n",
    "\n",
    "# INSTANTIATING a logistic regression model\n",
    "logreg = LogisticRegression(solver = 'lbfgs',\n",
    "                            C = 1,\n",
    "                            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "logreg_fit = logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "logreg_pred = logreg_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LogReg Training ACCURACY:', logreg_fit.score(X_train, y_train).round(4))\n",
    "print('LogReg Testing  ACCURACY:', logreg_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = logreg_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = logreg_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('LogReg Train-Test Gap   :', abs(logreg_train_score - logreg_test_score).round(4))\n",
    "logreg_test_gap = abs(logreg_train_score - logreg_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 64  92]\n",
      " [ 32 299]]\n"
     ]
    }
   ],
   "source": [
    "# creating a confusion matrix\n",
    "print(confusion_matrix(y_true = y_test,\n",
    "                       y_pred = logreg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 64\n",
      "False Positives: 92\n",
      "False Negatives: 32\n",
      "True Positives : 299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "logreg_tn, \\\n",
    "logreg_fp, \\\n",
    "logreg_fn, \\\n",
    "logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {logreg_tn}\n",
    "False Positives: {logreg_fp}\n",
    "False Negatives: {logreg_fn}\n",
    "True Positives : {logreg_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# calling the visual_cm function\n",
    "#visual_cm(true_y = y_test,\n",
    "#          pred_y = logreg_pred,\n",
    "#          labels = ['Cross Sell', 'No Cross Sell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6568\n"
     ]
    }
   ],
   "source": [
    "# area under the roc curve (auc) for logreg\n",
    "print(roc_auc_score(y_true  = y_test,\n",
    "                    y_score = logreg_pred).round(decimals = 4))\n",
    "\n",
    "\n",
    "# saving AUC score for future use\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = logreg_pred).round(decimals = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "logreg_model_values = zip(chef_total[chef1_dict['logit_sig_2']].columns,\n",
    "                          logreg_fit.coef_.ravel().round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "logreg_model_lst = [('intercept', logreg_fit.intercept_[0].round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "#for val in logreg_model_values:\n",
    "#    logreg_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "#for pair in logreg_model_lst:\n",
    "#    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification Trees (CART Models)</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>User Defined Function Created in Class</h4><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the function we created in class that will help us display the tree and plot the importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# User defined functions (display_tree and plot_feature_importance)\n",
    "\n",
    "########################################\n",
    "# display_tree\n",
    "########################################\n",
    "def display_tree(tree, feature_df, height = 500, width = 800):\n",
    "    \"\"\"\n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    tree       : fitted tree model object\n",
    "        fitted CART model to visualized\n",
    "    feature_df : DataFrame\n",
    "        DataFrame of explanatory features (used to generate labels)\n",
    "    height     : int, default 500\n",
    "        height in pixels to which to constrain image in html\n",
    "    width      : int, default 800\n",
    "        width in pixels to which to constrain image in html\n",
    "    \"\"\"\n",
    "\n",
    "    # visualizing the tree\n",
    "    dot_data = StringIO()\n",
    "\n",
    "    \n",
    "    # exporting tree to graphviz\n",
    "    export_graphviz(decision_tree      = tree,\n",
    "                    out_file           = dot_data,\n",
    "                    filled             = True,\n",
    "                    rounded            = True,\n",
    "                    special_characters = True,\n",
    "                    feature_names      = feature_df.columns)\n",
    "\n",
    "\n",
    "    # declaring a graph object\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "\n",
    "\n",
    "    # creating image\n",
    "    img = Image(graph.create_png(),\n",
    "                height = height,\n",
    "                width  = width)\n",
    "    \n",
    "    return img\n",
    "\n",
    "########################################\n",
    "# plot_feature_importances\n",
    "########################################\n",
    "def plot_feature_importances(model, train, export = False):\n",
    "    \"\"\"\n",
    "    Plots the importance of features from a CART model.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    model  : CART model\n",
    "    train  : explanatory variable training data\n",
    "    export : whether or not to export as a .png image, default False\n",
    "    \"\"\"\n",
    "    \n",
    "    # declaring the number\n",
    "    n_features = X_train.shape[1]\n",
    "    \n",
    "    # setting plot window\n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(pd.np.arange(n_features), train.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    \n",
    "    if export == True:\n",
    "        plt.savefig('Tree_Leaf_50_Feature_Importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run a classification tree model. It's likely to overfit the data since we won't restrict it's growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Training ACCURACY: 1.0\n",
      "Full Tree Testing ACCURACY : 0.6653\n",
      "Full Tree AUC Score: 0.6199\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "full_tree = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "full_tree_fit = full_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "full_tree_pred = full_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Full Tree Training ACCURACY:', full_tree_fit.score(X_train,\n",
    "                                                     y_train).round(4))\n",
    "\n",
    "print('Full Tree Testing ACCURACY :', full_tree_fit.score(X_test,\n",
    "                                                     y_test).round(4))\n",
    "\n",
    "print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,\n",
    "                                            y_score = full_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "full_tree_train_score = full_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "full_tree_test_score  = full_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC\n",
    "full_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                      y_score = full_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 79\n",
      "False Positives: 77\n",
      "False Negatives: 82\n",
      "True Positives : 249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "full_tree_tn, \\\n",
    "full_tree_fp, \\\n",
    "full_tree_fn, \\\n",
    "full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {full_tree_tn}\n",
    "False Positives: {full_tree_fp}\n",
    "False Negatives: {full_tree_fn}\n",
    "True Positives : {full_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# calling display_tree\n",
    "#display_tree(tree       = full_tree_fit,\n",
    "#             feature_df = X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New classification Tree with a maximum depth of 4 and a minimum number of samples per leaf of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7402\n",
      "Testing  ACCURACY: 0.7762\n",
      "AUC Score        : 0.732\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "pruned_tree = DecisionTreeClassifier(max_depth = 3,\n",
    "                                     min_samples_leaf = 25,\n",
    "                                     random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "pruned_tree_fit  = pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "pruned_tree_pred = pruned_tree_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the model\n",
    "print('Training ACCURACY:', pruned_tree_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', pruned_tree_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = pruned_tree_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "pruned_tree_train_score = pruned_tree_fit.score(X_train, y_train).round(4) # accuracy\n",
    "pruned_tree_test_score  = pruned_tree_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving auc score\n",
    "pruned_tree_auc_score   = roc_auc_score(y_true  = y_test,\n",
    "                                        y_score = pruned_tree_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 95\n",
      "False Positives: 61\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "pruned_tree_tn, \\\n",
    "pruned_tree_fp, \\\n",
    "pruned_tree_fn, \\\n",
    "pruned_tree_tp = confusion_matrix(y_true = y_test, y_pred = pruned_tree_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {pruned_tree_tn}\n",
    "False Positives: {pruned_tree_fp}\n",
    "False Negatives: {pruned_tree_fn}\n",
    "True Positives : {pruned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# calling display_tree\n",
    "#display_tree(tree       = pruned_tree_fit,\n",
    "#             feature_df = X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plotting feature importance\n",
    "#plot_feature_importances(pruned_tree_fit,\n",
    "#                         train = X_train,\n",
    "#                         export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the feature importance of the CARt model we can observe that Junk and number_of_names where the features most important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Most likley remove the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# comparing results and creating performance df\n",
    "\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Name'    : ['Logistic', 'Full Tree', 'Pruned Tree'],\n",
    "           \n",
    "    'AUC Score' : [logreg_auc_score, full_tree_auc_score, pruned_tree_auc_score],\n",
    "    \n",
    "    'Training Accuracy' : [logreg_train_score, full_tree_train_score,\n",
    "                           pruned_tree_train_score],\n",
    "           \n",
    "    'Testing Accuracy'  : [logreg_test_score, full_tree_test_score,\n",
    "                           pruned_tree_test_score],\n",
    "\n",
    "    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp),\n",
    "                           (full_tree_tn, full_tree_fp, full_tree_fn, full_tree_tp),\n",
    "                           (pruned_tree_tn, pruned_tree_fp, pruned_tree_fn, pruned_tree_tp)]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "\n",
    "# sending model results to Excel\n",
    "#model_performance.to_excel('./classification_model_performance.xlsx',\n",
    "#                           index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Logistic Regression with Default Hyperparameters</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a logistic regression model with default values\n",
    "lr_default = LogisticRegression(solver = 'lbfgs',\n",
    "                                C = 1.0,\n",
    "                                warm_start = False,\n",
    "                                random_state = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7457\n",
      "Testing  ACCURACY: 0.7454\n",
      "AUC Score        : 0.6568\n"
     ]
    }
   ],
   "source": [
    "# FITTING the training data\n",
    "lr_default_fit = lr_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_default_pred = lr_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', lr_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', lr_default_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# SCORING with AUC\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_default_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "logreg_train_score = lr_default_fit.score(X_train, y_train).round(4) # accuracy\n",
    "logreg_test_score  = lr_default_fit.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "logreg_auc_score = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = lr_default_pred).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Tuning with RandomizedSearchCV</h3><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part instead of manually fine tuning our model to have the best AUC, we automate the process with RandomizedSearchCV. This process will automatically go through a large number of iterations to output the hyperparameters with the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# # RandomizedSearchCV\n",
    "# ########################################\n",
    "\n",
    "# # declaring a hyperparameter space\n",
    "# C_space          = pd.np.arange(0.1, 5.0, 0.1)\n",
    "# warm_start_space = [True, False]\n",
    "# solver_space     = ['newton-cg', 'sag', 'lbfgs']\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'C'          : C_space,\n",
    "#               'warm_start' : warm_start_space,\n",
    "#               'solver'     : solver_space}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# lr_tuned = LogisticRegression(random_state = 219,\n",
    "#                               max_iter     = 1000)\n",
    "\n",
    "\n",
    "# # GridSearchCV object\n",
    "# lr_tuned_cv = RandomizedSearchCV(estimator           = lr_tuned,   # the model object\n",
    "#                                  param_distributions = param_grid, # parameters to tune\n",
    "#                                  cv                  = 3,          # how many folds in cross-validation\n",
    "#                                  n_iter              = 999,        # number of combinations of hyperparameters to try\n",
    "#                                  random_state        = 219,        # starting point for random sequence\n",
    "#                                  scoring = make_scorer(\n",
    "#                                            roc_auc_score,\n",
    "#                                            needs_threshold = False)) # scoring criteria (AUC)\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# lr_tuned_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", lr_tuned_cv.best_params_)\n",
    "# print(\"Tuned CV AUC      :\", lr_tuned_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.9000000000000001, max_iter=1000, random_state=219,\n",
       "                   solver='newton-cg', warm_start=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the best estimator for the model\n",
    "#    lr_tuned_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Tuned Training ACCURACY: 0.7409\n",
      "LR Tuned Testing  ACCURACY: 0.7413\n",
      "LR Tuned AUC Score        : 0.6538\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "lr_tuned = LogisticRegression(C=1.9000000000000001, max_iter=1000, random_state=219,\n",
    "                   solver='newton-cg', warm_start=True)\n",
    "\n",
    "\n",
    "\n",
    "# Fit\n",
    "lr_tuned.fit(chef_data, chef_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "lr_tuned_pred = lr_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('LR Tuned Training ACCURACY:', lr_tuned.score(X_train, y_train).round(4))\n",
    "print('LR Tuned Testing  ACCURACY:', lr_tuned.score(X_test, y_test).round(4))\n",
    "print('LR Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = lr_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_tuned_train_score = lr_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "lr_tuned_test_score  = lr_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "lr_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = lr_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 64\n",
      "False Positives: 92\n",
      "False Negatives: 34\n",
      "True Positives : 297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "lr_tuned_tn, \\\n",
    "lr_tuned_fp, \\\n",
    "lr_tuned_fn, \\\n",
    "lr_tuned_tp = confusion_matrix(y_true = y_test, y_pred = lr_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {lr_tuned_tn}\n",
    "False Positives: {lr_tuned_fp}\n",
    "False Negatives: {lr_tuned_fn}\n",
    "True Positives : {lr_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Saving model performance\n",
    "#model_performance = pd.read_excel('./classification_model_performance.xlsx')\n",
    "\n",
    "\n",
    "# declaring model performance objects\n",
    "lr_train_acc = lr_tuned.score(X_train, y_train).round(4)\n",
    "lr_test_acc  = lr_tuned.score(X_test, y_test).round(4)\n",
    "lr_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = lr_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned LR',\n",
    "                           'Training Accuracy' : lr_train_acc,\n",
    "                           'Testing Accuracy'  : lr_test_acc,\n",
    "                           'AUC Score'         : lr_auc,\n",
    "                           'Confusion Matrix'  : (lr_tuned_tn,\n",
    "                                                  lr_tuned_fp,\n",
    "                                                  lr_tuned_fn,\n",
    "                                                  lr_tuned_tp)},\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "# model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Hyperparameter Tuning on Classification Trees</h3><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-100-4dc3f988466f>:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  depth_space     = pd.np.arange(1, 25, 1)\n",
      "<ipython-input-100-4dc3f988466f>:5: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  leaf_space      = pd.np.arange(1, 100, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters  : {'splitter': 'best', 'min_samples_leaf': 16, 'max_depth': 3, 'criterion': 'gini'}\n",
      "Tuned Training AUC: 0.7032\n"
     ]
    }
   ],
   "source": [
    "# # declaring a hyperparameter space\n",
    "# criterion_space = ['gini', 'entropy']\n",
    "# splitter_space  = ['best', 'random']\n",
    "# depth_space     = pd.np.arange(1, 25, 1)\n",
    "# leaf_space      = pd.np.arange(1, 100, 1)\n",
    "\n",
    "\n",
    "# # creating a hyperparameter grid\n",
    "# param_grid = {'criterion'        : criterion_space,\n",
    "#               'splitter'         : splitter_space,\n",
    "#               'max_depth'        : depth_space,\n",
    "#               'min_samples_leaf' : leaf_space}\n",
    "\n",
    "\n",
    "# # INSTANTIATING the model object without hyperparameters\n",
    "# tuned_tree = DecisionTreeClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# # RandomizedSearchCV object\n",
    "# tuned_tree_cv = RandomizedSearchCV(estimator             = tuned_tree,\n",
    "#                                    param_distributions   = param_grid,\n",
    "#                                    cv                    = 3,\n",
    "#                                    n_iter                = 1000,\n",
    "#                                    random_state          = 219,\n",
    "#                                    scoring = make_scorer(roc_auc_score,\n",
    "#                                              needs_threshold = False))\n",
    "\n",
    "\n",
    "# # FITTING to the FULL DATASET (due to cross-validation)\n",
    "# tuned_tree_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# # PREDICT step is not needed\n",
    "\n",
    "\n",
    "# # printing the optimal parameters and best score\n",
    "# print(\"Tuned Parameters  :\", tuned_tree_cv.best_params_)\n",
    "# print(\"Tuned Training AUC:\", tuned_tree_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3, min_samples_leaf=16, random_state=219)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tuned_tree_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7402\n",
      "Testing  ACCURACY: 0.7762\n",
      "AUC Score        : 0.732\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a logistic regression model with tuned values\n",
    "tree_tuned = DecisionTreeClassifier(max_depth=3, min_samples_leaf=16, random_state=219)\n",
    "\n",
    "\n",
    "# FIT step is not needed\n",
    "tree_tuned.fit(chef_data, chef_target)\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "tree_tuned_pred = tree_tuned.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', tree_tuned.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', tree_tuned.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = tree_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "tree_tuned_train_score = tree_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "tree_tuned_test_score  = tree_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "tree_tuned_auc         = roc_auc_score(y_true  = y_test,\n",
    "                                     y_score = tree_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 95\n",
      "False Positives: 61\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_tree_tn, \\\n",
    "tuned_tree_fp, \\\n",
    "tuned_tree_fn, \\\n",
    "tuned_tree_tp = confusion_matrix(y_true = y_test, y_pred = tree_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_tree_tn}\n",
    "False Positives: {tuned_tree_fp}\n",
    "False Negatives: {tuned_tree_fn}\n",
    "True Positives : {tuned_tree_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "tree_train_acc = tree_tuned.score(X_train, y_train).round(4)\n",
    "tree_test_acc  = tree_tuned.score(X_test, y_test).round(4)\n",
    "tree_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = tree_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned Tree',\n",
    "                           'Training Accuracy' : tree_train_acc,\n",
    "                           'Testing Accuracy'  : tree_test_acc,\n",
    "                           'AUC Score'         : tree_auc,\n",
    "                           'Confusion Matrix'  : (tuned_tree_tn,\n",
    "                                                  tuned_tree_fp,\n",
    "                                                  tuned_tree_fn,\n",
    "                                                  tuned_tree_tp)},\n",
    "                           ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "#model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# displaying the tree\n",
    "#display_tree(tree = tree_tuned,\n",
    "#             feature_df = chef_data,\n",
    "#             height = 1500,\n",
    "#             width  = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# saving the DataFrame to Excel\n",
    "#model_performance.to_excel('./classification_model_performance.xlsx',\n",
    "#                           index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Random Forest</h3><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a random forest model with default values\n",
    "rf_default = RandomForestClassifier(n_estimators     = 100,\n",
    "                                    criterion        = 'gini',\n",
    "                                    max_depth        = None,\n",
    "                                    min_samples_leaf = 1,\n",
    "                                    bootstrap        = True,\n",
    "                                    warm_start       = False,\n",
    "                                    random_state     = 219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 1.0\n",
      "Testing  ACCURACY: 0.7433\n",
      "AUC Score        : 0.6688\n"
     ]
    }
   ],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', rf_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', rf_default_fit.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving AUC score\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = rf_default_fit_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plotting feature importances\n",
    "#plot_feature_importances(rf_default_fit,\n",
    "#                         train = X_train,\n",
    "#                         export = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot feature importance we can see that the most important feature for this model id the avg time per site visit times the avg prep video time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 72\n",
      "False Positives: 84\n",
      "False Negatives: 41\n",
      "True Positives : 290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "rf_tn, \\\n",
    "rf_fp, \\\n",
    "rf_fn, \\\n",
    "rf_tp = confusion_matrix(y_true = y_test, y_pred = rf_default_fit_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {rf_tn}\n",
    "False Positives: {rf_fp}\n",
    "False Negatives: {rf_fn}\n",
    "True Positives : {rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "rf_train_acc = rf_default_fit.score(X_train, y_train).round(4)\n",
    "rf_test_acc  = rf_default_fit.score(X_test, y_test).round(4)\n",
    "rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                             y_score = rf_default_fit_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Random Forest (Full)',\n",
    "                           'Training Accuracy'  : rf_train_acc,\n",
    "                           'Testing Accuracy'   : rf_test_acc,\n",
    "                           'AUC Score'          : rf_auc,\n",
    "                           'Confusion Matrix'   : (rf_tn,\n",
    "                                                   rf_fp,\n",
    "                                                   rf_fn,\n",
    "                                                   rf_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "#model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use GridSearchCV again to iterate over 1000 trees to find hyperparameter of the model that perfomed the best. The output will be used manually so we don't have to run GridSearch each time we run our script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-78-d8e7c111503d>:10: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  estimator_space  = pd.np.arange(100, 1100, 250)\n",
      "<ipython-input-78-d8e7c111503d>:11: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  leaf_space       = pd.np.arange(1, 31, 10)\n"
     ]
    }
   ],
   "source": [
    "# FITTING the training data\n",
    "rf_default_fit = rf_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "rf_default_fit_pred = rf_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "estimator_space  = pd.np.arange(100, 1100, 250)\n",
    "leaf_space       = pd.np.arange(1, 31, 10)\n",
    "criterion_space  = ['gini', 'entropy']\n",
    "bootstrap_space  = [True, False]\n",
    "warm_start_space = [True, False]\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'n_estimators'     : estimator_space,\n",
    "              'min_samples_leaf' : leaf_space,\n",
    "              'criterion'        : criterion_space,\n",
    "              'bootstrap'        : bootstrap_space,\n",
    "              'warm_start'       : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "forest_grid = RandomForestClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "# forest_cv = RandomizedSearchCV(estimator           = forest_grid,\n",
    "#                               param_distributions = param_grid,\n",
    "#                               cv         = 3,\n",
    "#                               n_iter     = 1000,\n",
    "#                               scoring    = make_scorer(roc_auc_score,\n",
    "#                                            needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#forest_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", forest_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", forest_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, criterion='entropy',\n",
       "                       min_samples_leaf=11, n_estimators=850, random_state=219,\n",
       "                       warm_start=True)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best estimators based on RandomizedSearchCV\n",
    "#forest_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest Tuned Training ACCURACY: 0.7916\n",
      "Forest Tuned Testing  ACCURACY: 0.8111\n",
      "Forest Tuned AUC Score        : 0.7373\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results we found in the previous cells\n",
    "\n",
    "# copy/pasting in the best_estimator_ results\n",
    "# to avoid running another RandomizedSearch\n",
    "forest_tuned = RandomForestClassifier(bootstrap=False, criterion='entropy',\n",
    "                       min_samples_leaf=11, n_estimators=850, random_state=219,\n",
    "                       warm_start=True)\n",
    "\n",
    "\n",
    "# FITTING the model object\n",
    "forest_tuned_fit = forest_tuned.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "forest_tuned_pred = forest_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Forest Tuned Training ACCURACY:', forest_tuned.score(X_train, y_train).round(4))\n",
    "print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(X_test, y_test).round(4))\n",
    "print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                                   y_score = forest_tuned_pred).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "forest_tuned_train_score = forest_tuned.score(X_train, y_train).round(4) # accuracy\n",
    "forest_tuned_test_score  = forest_tuned.score(X_test, y_test).round(4)   # accuracy\n",
    "\n",
    "\n",
    "# saving the AUC score\n",
    "forest_tuned_auc = roc_auc_score(y_true  = y_test,\n",
    "                                 y_score = forest_tuned_pred).round(4) # auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 70\n",
      "False Positives: 86\n",
      "False Negatives: 29\n",
      "True Positives : 302\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "tuned_rf_tn, \\\n",
    "tuned_rf_fp, \\\n",
    "tuned_rf_fn, \\\n",
    "tuned_rf_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {tuned_rf_tn}\n",
    "False Positives: {tuned_rf_fp}\n",
    "False Negatives: {tuned_rf_fn}\n",
    "True Positives : {tuned_rf_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects and appending to model_performance\n",
    "tuned_rf_train_acc = forest_tuned_fit.score(X_train, y_train).round(4)\n",
    "tuned_rf_test_acc  = forest_tuned_fit.score(X_test, y_test).round(4)\n",
    "tuned_rf_auc       = roc_auc_score(y_true  = y_test,\n",
    "                                   y_score = forest_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'         : 'Tuned Random Forest (Full)',\n",
    "                           'Training Accuracy'  : tuned_rf_train_acc,\n",
    "                           'Testing Accuracy'   : tuned_rf_test_acc,\n",
    "                           'AUC Score'          : tuned_rf_auc,\n",
    "                           'Confusion Matrix'   : (tuned_rf_tn,\n",
    "                                                   tuned_rf_fp,\n",
    "                                                   tuned_rf_fn,\n",
    "                                                   tuned_rf_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "#model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient Boosted Machines</h3><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.9918\n",
      "Testing ACCURACY : 0.7474\n",
      "AUC Score        : 0.6871\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_default = GradientBoostingClassifier(loss          = 'deviance',\n",
    "                                              learning_rate = 0.1,\n",
    "                                              n_estimators  = 100,\n",
    "                                              criterion     = 'friedman_mse',\n",
    "                                              max_depth     = 8,\n",
    "                                              warm_start    = False,\n",
    "                                              random_state  = 219)\n",
    "\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "full_gbm_default_fit = full_gbm_default.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "full_gbm_default_pred = full_gbm_default_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', full_gbm_default_fit.score(X_train, y_train).round(4))\n",
    "print('Testing ACCURACY :', full_gbm_default_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = full_gbm_default_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 81\n",
      "False Positives: 75\n",
      "False Negatives: 48\n",
      "True Positives : 283\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_default_tn, \\\n",
    "gbm_default_fp, \\\n",
    "gbm_default_fn, \\\n",
    "gbm_default_tp = confusion_matrix(y_true = y_test, y_pred = full_gbm_default_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_default_tn}\n",
    "False Positives: {gbm_default_fp}\n",
    "False Negatives: {gbm_default_fn}\n",
    "True Positives : {gbm_default_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = full_gbm_default_fit.score(X_train, y_train).round(4)\n",
    "gbm_test_acc  = full_gbm_default_fit.score(X_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = full_gbm_default_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'       : 'GBM (Full)',\n",
    "                          'Training Accuracy' : gbm_train_acc,\n",
    "                          'Testing Accuracy'  : gbm_test_acc,\n",
    "                          'AUC Score'         : gbm_auc,\n",
    "                          'Confusion Matrix'  : (gbm_default_tn,\n",
    "                                                 gbm_default_fp,\n",
    "                                                 gbm_default_fn,\n",
    "                                                 gbm_default_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "# model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gradient boosting, we use the same startegy with GridSearch that we used for the previous model. Gridsearch needs to only be ran once and we can save the best parameters and manually input them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-87-e459a33c90f0>:2: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  learn_space        = pd.np.arange(0.1, 2.0, 0.2)\n",
      "<ipython-input-87-e459a33c90f0>:3: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  estimator_space    = pd.np.arange(100, 200, 25)\n",
      "<ipython-input-87-e459a33c90f0>:4: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  depth_space        = pd.np.arange(1, 20, 2)\n"
     ]
    }
   ],
   "source": [
    "# declaring a hyperparameter space\n",
    "learn_space        = pd.np.arange(0.1, 2.0, 0.2)\n",
    "estimator_space    = pd.np.arange(100, 200, 25)\n",
    "depth_space        = pd.np.arange(1, 20, 2)\n",
    "warm_start_space   = [True, False]\n",
    "\n",
    "# creating a hyperparameter grid\n",
    "param_grid = {'learning_rate' : learn_space,\n",
    "              'max_depth'     : depth_space,\n",
    "              'n_estimators'  : estimator_space,\n",
    "              'warm_start'     : warm_start_space}\n",
    "\n",
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "full_gbm_grid = GradientBoostingClassifier(random_state = 219)\n",
    "\n",
    "\n",
    "# GridSearchCV object\n",
    "#full_gbm_cv = RandomizedSearchCV(estimator     = full_gbm_grid,\n",
    "#                           param_distributions = param_grid,\n",
    "#                           cv                  = 3,\n",
    "#                           n_iter              = 500,\n",
    "#                           random_state        = 219,\n",
    "#                           scoring             = make_scorer(roc_auc_score,\n",
    "#                                                 needs_threshold = False))\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "#full_gbm_cv.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICT step is not needed\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "#print(\"Tuned Parameters  :\", full_gbm_cv.best_params_)\n",
    "#print(\"Tuned Training AUC:\", full_gbm_cv.best_score_.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# checking the best estimator for the model\n",
    "#full_gbm_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ACCURACY: 0.7985\n",
      "Testing  ACCURACY: 0.8049\n",
      "AUC Score        : 0.7379\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING the model object without hyperparameters\n",
    "\n",
    "# I made several attempts to hyperparameter tuning\n",
    "gbm_tuned = GradientBoostingClassifier(random_state=219, warm_start=True)\n",
    "\n",
    "# FIT step is needed as we are not using .best_estimator\n",
    "gbm_tuned_fit = gbm_tuned.fit(chef_data, chef_target)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "gbm_tuned_pred = gbm_tuned_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Training ACCURACY:', gbm_tuned_fit.score(X_train, y_train).round(4))\n",
    "print('Testing  ACCURACY:', gbm_tuned_fit.score(X_test, y_test).round(4))\n",
    "print('AUC Score        :', roc_auc_score(y_true  = y_test,\n",
    "                                          y_score = gbm_tuned_pred).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "True Negatives : 86\n",
      "False Positives: 70\n",
      "False Negatives: 25\n",
      "True Positives : 306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unpacking the confusion matrix\n",
    "gbm_tuned_tn, \\\n",
    "gbm_tuned_fp, \\\n",
    "gbm_tuned_fn, \\\n",
    "gbm_tuned_tp = confusion_matrix(y_true = y_test, y_pred = gbm_tuned_pred).ravel()\n",
    "\n",
    "\n",
    "# printing each result one-by-one\n",
    "print(f\"\"\"\n",
    "True Negatives : {gbm_tuned_tn}\n",
    "False Positives: {gbm_tuned_fp}\n",
    "False Negatives: {gbm_tuned_fn}\n",
    "True Positives : {gbm_tuned_tp}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# declaring model performance objects\n",
    "gbm_train_acc = gbm_tuned_fit.score(X_train, y_train).round(4)\n",
    "gbm_test_acc  = gbm_tuned_fit.score(X_test, y_test).round(4)\n",
    "gbm_auc       = roc_auc_score(y_true  = y_test,\n",
    "                              y_score = gbm_tuned_pred).round(4)\n",
    "\n",
    "\n",
    "# appending to model_performance\n",
    "model_performance = model_performance.append(\n",
    "                          {'Model Name'        : 'Tuned GBM',\n",
    "                          'Training Accuracy'  : gbm_train_acc,\n",
    "                          'Testing Accuracy'   : gbm_test_acc,\n",
    "                          'AUC Score'          : gbm_auc,\n",
    "                          'Confusion Matrix'   : (gbm_tuned_tn,\n",
    "                                                  gbm_tuned_fp,\n",
    "                                                  gbm_tuned_fn,\n",
    "                                                  gbm_tuned_tp)},\n",
    "                          ignore_index = True)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "#model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Conclusion</h2><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we display the AUC score of all of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Score</th>\n",
       "      <th>Training Accuracy</th>\n",
       "      <th>Testing Accuracy</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Tuned GBM</td>\n",
       "      <td>0.7379</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>(86, 70, 25, 306)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pruned Tree</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned Tree</td>\n",
       "      <td>0.7320</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>0.7762</td>\n",
       "      <td>(95, 61, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GBM (Full)</td>\n",
       "      <td>0.6871</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(81, 75, 48, 283)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tuned Random Forest (Full)</td>\n",
       "      <td>0.6806</td>\n",
       "      <td>0.7930</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>(70, 86, 29, 302)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest (Full)</td>\n",
       "      <td>0.6688</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7433</td>\n",
       "      <td>(72, 84, 41, 290)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tuned GBM</td>\n",
       "      <td>0.6634</td>\n",
       "      <td>0.8067</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>(67, 89, 34, 297)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.6568</td>\n",
       "      <td>0.7457</td>\n",
       "      <td>0.7454</td>\n",
       "      <td>(64, 92, 32, 299)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned LR</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.7409</td>\n",
       "      <td>0.7413</td>\n",
       "      <td>(64, 92, 34, 297)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Full Tree</td>\n",
       "      <td>0.6293</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6735</td>\n",
       "      <td>(79, 77, 82, 249)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model Name  AUC Score  Training Accuracy  Testing Accuracy   Confusion Matrix\n",
       "9                   Tuned GBM     0.7379             0.7985            0.8049  (86, 70, 25, 306)\n",
       "2                 Pruned Tree     0.7320             0.7402            0.7762  (95, 61, 48, 283)\n",
       "4                  Tuned Tree     0.7320             0.7402            0.7762  (95, 61, 48, 283)\n",
       "7                  GBM (Full)     0.6871             0.9918            0.7474  (81, 75, 48, 283)\n",
       "6  Tuned Random Forest (Full)     0.6806             0.7930            0.7639  (70, 86, 29, 302)\n",
       "5        Random Forest (Full)     0.6688             1.0000            0.7433  (72, 84, 41, 290)\n",
       "8                   Tuned GBM     0.6634             0.8067            0.7474  (67, 89, 34, 297)\n",
       "0                    Logistic     0.6568             0.7457            0.7454  (64, 92, 32, 299)\n",
       "3                    Tuned LR     0.6538             0.7409            0.7413  (64, 92, 34, 297)\n",
       "1                   Full Tree     0.6293             1.0000            0.6735  (79, 77, 82, 249)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_performance.sort_values(by = 'AUC Score',\n",
    "                              ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the Gradient Boosting Machine tuned tree has the highest AUC. It has a score of 0.7379. For that reason to predict which customer will buy the Halfway There Subscription we can use this model.\n",
    "\n",
    "My selected model is <b>Tuned GBM</b> with an <b>AUC score </b>of <b>0.7379.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# We can save our results to excel by removing the comments\n",
    "# model_performance.to_excel('./classification_model_performance.xlsx',\n",
    "#                           index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
